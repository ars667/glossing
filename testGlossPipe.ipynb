{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNBtMGhWXVMV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers transliterate googletrans --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEoYI4e-XQ6z",
        "outputId": "e0e0c70b-a037-4302-b84c-9debaa57795c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x10TRy2FIlV8"
      },
      "outputs": [],
      "source": [
        "from transliterate import *\n",
        "from googletrans import Translator\n",
        "import transformers\n",
        "from transformers import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "b22ce7bf9a444a0a9ad1be521cda68fe",
            "afa78841ffd749efb02104ce643b2fd5",
            "96588452b81749ee9f67f2d2dd8f541e"
          ]
        },
        "id": "MZhQPmKEInXR",
        "outputId": "f9a13146-ba6c-4446-fd77-b5ff95cf590c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file /content/drive/My Drive/fine_tuned_t5/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 3968,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 1536,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 18,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"ByT5Tokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 384\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/My Drive/fine_tuned_t5/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/My Drive/fine_tuned_t5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file /content/drive/My Drive/fine_tuned_t5/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/My Drive/albert/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/My Drive/albert/model.safetensors\n",
            "All model checkpoint weights were used when initializing AlbertForMaskedLM.\n",
            "\n",
            "All the weights of AlbertForMaskedLM were initialized from the model checkpoint at /content/drive/My Drive/albert.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForMaskedLM for predictions without further training.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b22ce7bf9a444a0a9ad1be521cda68fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afa78841ffd749efb02104ce643b2fd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--byt5-base/snapshots/92d8c008d55cf7c254915bac165171dfe6c20c44/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--byt5-base/snapshots/92d8c008d55cf7c254915bac165171dfe6c20c44/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file chat_template.jinja from cache at None\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96588452b81749ee9f67f2d2dd8f541e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--byt5-base/snapshots/92d8c008d55cf7c254915bac165171dfe6c20c44/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 3968,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 1536,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 18,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"ByT5Tokenizer\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 384\n",
            "}\n",
            "\n",
            "loading file spiece.model\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading file tokenizer.json\n",
            "loading file chat_template.jinja\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from transformers import AlbertForMaskedLM, AlbertTokenizer\n",
        "from transliterate import translit\n",
        "from googletrans import Translator\n",
        "\n",
        "class TextChanger:\n",
        "    def replace_until_next_word(self, text):\n",
        "        result = []\n",
        "        i = 0\n",
        "        n = len(text)\n",
        "        tmp = ''\n",
        "        masks = []\n",
        "        while i < n:\n",
        "            # Если находим символ '-'\n",
        "            if text[i] == '-' or text[i] == '.':\n",
        "                # Заменяем все символы до следующего пробела или конца строки на '_'\n",
        "                while i < n and text[i] != ' ':\n",
        "                    tmp += text[i]\n",
        "                    result.append('_')\n",
        "                    i += 1\n",
        "            else:\n",
        "                # Если это не '-', просто добавляем символ в результат\n",
        "                result.append(text[i])\n",
        "                if tmp != '':\n",
        "                    masks.append(tmp)\n",
        "                tmp = ''\n",
        "                i += 1\n",
        "\n",
        "        # Собираем результат в строку\n",
        "        return \"\".join(result)\n",
        "\n",
        "    def replace_underscores_with_mask(self, text):\n",
        "        return re.sub(r\"_{2,}\", \" [MASK]\", text)\n",
        "\n",
        "    def generate_masks_input(self, text, tokenizer):\n",
        "        return tokenizer(self.replace_underscores_with_mask(self.replace_until_next_word(text)), return_tensors=\"pt\")\n",
        "\n",
        "    def generate_gloss_text(self, tokenizer, transcription, translation, lang=\"Ijor\", metalang=\"English\", is_segmented=False):\n",
        "        return tokenizer(f\"\"\"Provide the glosses for the following transcription in {lang}.\n",
        "\n",
        "        Transcription in {lang}: {transcription}\n",
        "        Transcription segmented: {is_segmented}\n",
        "        Translation in {metalang}: {translation}\\n\n",
        "        Glosses:\n",
        "        \"\"\", return_tensors=\"pt\")\n",
        "\n",
        "class ModelLoader:\n",
        "    def load_gloss_model(self, path_to_model=\"/content/drive/My Drive/fine_tuned_t5\"):\n",
        "        my_model = transformers.T5ForConditionalGeneration.from_pretrained(path_to_model)\n",
        "        return my_model\n",
        "\n",
        "    def load_gloss_tokenizer(self, path_to_tokenizer=\"google/byt5-base\"):\n",
        "        tokenizer = transformers.ByT5Tokenizer.from_pretrained(path_to_tokenizer, use_fast=False)\n",
        "        return tokenizer\n",
        "\n",
        "    def load_mask_model(self, path_to_model=\"/content/drive/My Drive/albert\"):\n",
        "        model = AlbertForMaskedLM.from_pretrained(path_to_model)\n",
        "        return model\n",
        "\n",
        "    def load_mask_tokenizer(self, path_to_tokenizer=\"/content/drive/My Drive/albert\"):\n",
        "        tokenizer = AlbertTokenizer.from_pretrained(path_to_tokenizer)\n",
        "        return tokenizer\n",
        "\n",
        "textChanger = TextChanger()\n",
        "modelLoader = ModelLoader()\n",
        "\n",
        "GlossModel = modelLoader.load_gloss_model()\n",
        "MaskModel = modelLoader.load_mask_model()\n",
        "tokenizerGlossModel = modelLoader.load_gloss_tokenizer()\n",
        "tokenizerMaskModel = modelLoader.load_mask_tokenizer()\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "i = input()\n",
        "transcription = translit(i, language_code='ru', reversed=True)\n",
        "result = await translator.translate(i)\n",
        "translation = result.text\n",
        "textToGlossModel = textChanger.generate_gloss_text(tokenizerGlossModel, i, transcription, translation)\n",
        "GlossModelAnswer = tokenizerGlossModel.batch_decode(\n",
        "    GlossModel.generate(**textToGlossModel, max_length=1024), skip_special_tokens=True\n",
        ")\n",
        "print(\"GLOSS MODEL ANSWER ->\", GlossModelAnswer[0])\n",
        "\n",
        "MaskTextFromGlossModel = textChanger.replace_underscores_with_mask(textChanger.replace_until_next_word(GlossModelAnswer[0]))\n",
        "textToMaskModel = textChanger.generate_masks_input(GlossModelAnswer[0], tokenizerMaskModel)\n",
        "\n",
        "# Получаем logits и inputs\n",
        "maskModelOutputs = MaskModel(**textToMaskModel)\n",
        "logits = maskModelOutputs.logits\n",
        "inputs = textToMaskModel\n",
        "\n",
        "# Находим все позиции масок\n",
        "mask_token_indices = (inputs['input_ids'] == tokenizerMaskModel.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "# Предсказание токенов для каждой маски\n",
        "predicted_tokens = []\n",
        "for mask_index in mask_token_indices:\n",
        "    predicted_token_id = logits[0, mask_index].argmax(axis=-1)\n",
        "    predicted_token = tokenizerMaskModel.decode(predicted_token_id)\n",
        "    predicted_tokens.append(predicted_token)\n",
        "\n",
        "# Замена масок на предсказанные токены\n",
        "predicted_text = MaskTextFromGlossModel\n",
        "for token in predicted_tokens:\n",
        "    predicted_text = predicted_text.replace(\"[MASK]\", token, 1)  # Заменяем только одну маску за раз\n",
        "\n",
        "print(\"TEXT TO MASK MODEL ->\")\n",
        "print(\"FINAL_ANSWER ->\", predicted_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiTkX6IMI3dw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}